---
layout: post
title: "LLM 快速推理的两种技术路线"
date: 2026-02-15 18:37:01 +0800
categories: tech-translation
description: "Anthropic 和 OpenAI 最近都推出了「快速模式」，但背后的技术实现截然不同：一个是低批次推理，另一个是 Cerebras 超大芯片。"
original_url: https://www.seangoedecke.com/fast-llm-inference/
source: Hacker News
---

本文翻译自 [Two different tricks for fast LLM inference](https://www.seangoedecke.com/fast-llm-inference/)，原载于 Hacker News。

---

Anthropic 和 OpenAI 最近都宣布了「快速模式」：一种以显著更高的速度与其最佳编程模型交互的方式。

这两个快速模式版本截然不同。Anthropic 的快速模式提供高达 2.5 倍的 token/s（约 170，相比 Opus 4.6 的 65）。OpenAI 的快速模式则提供超过 1000 token/s（相比 GPT-5.3-Codex 的 65 token/s，提升了 15 倍）。所以 OpenAI 的快速模式比 Anthropic 的快六倍。

然而，Anthropic 的巨大优势在于他们提供的是真正的模型。当你使用他们的快速模式时，你得到的是真正的 Opus 4.6，而当你使用 OpenAI 的快速模式时，你得到的是 GPT-5.3-Codex-Spark，而不是真正的 GPT-5.3-Codex。Spark 确实快得多，但它是一个明显更弱的模型：对于许多任务来说足够好，但它会犯错、搞砸工具调用——这些是原版 GPT-5.3-Codex 绝不会做的。

为什么会有这些差异？AI 实验室并没有宣传他们快速模式的工作细节，但我相当确信是这样的：**Anthropic 的快速模式基于低批次推理，而 OpenAI 的快速模式基于特殊的 Cerebras 超大芯片**。让我详细解释一下。

## Anthropic 快速模式的工作原理

AI 推理经济学核心的权衡是 **batching（批处理）**，因为主要瓶颈是**内存**。GPU 非常快，但将数据移动到 GPU 上并不快。每次推理操作都需要在推理开始之前将用户 prompt 的所有 token 复制到 GPU 上。因此，将多个用户批处理在一起会增加整体吞吐量，但代价是让用户等待批次填满。

一个很好的类比是公交系统。如果你对乘客实行零批处理——即每当有人上车，公交车就立即出发——对于那些设法上车的人来说，通勤会快得多。但显然整体吞吐量会低得多，因为人们会在公交车站等上几个小时才能真正上车。

Anthropic 的快速模式基本上是一张公交通行证，保证你一上车公交车就立即离开。它的价格是六倍，因为你实际上是在为所有其他本可以和你一起上车的人付费，但它快得多，因为你花费**零**时间等待公交车离开。

显然我不能完全确定这是正确的。也许他们有一些新的超快速计算资源，或者他们正在做一些别人没想到的算法技巧。但我相当确定就是这样。全新的计算资源或算法技巧可能需要对模型进行更改（参见下面 OpenAI 的系统），而「六倍价格换 2.5 倍速度」正是你期望从切换到低批次模式中获得的改进范围。

## OpenAI 快速模式的工作原理

OpenAI 的快速模式完全不是这样工作的。你可以简单地从他们为此引入了一个新的、更弱的模型这一点看出来。如果他们只是调整批次大小，完全没有理由这样做。而且，他们在公告博客文章中明确告诉了我们支持快速模式的是什么：Cerebras。

OpenAI 在一月份（一个月前）宣布了他们的 Cerebras 合作伙伴关系。Cerebras 是什么？他们制造「超低延迟计算」。实际上，这意味着他们制造**巨型芯片**。H100 芯片（相当接近前沿推理芯片）只有一平方英寸多一点。而 Cerebras 芯片是 **70 平方英寸**。

你可以从图片中看到 Cerebras 芯片上有网格和孔洞的图案。那是因为这么大的硅晶圆本应该被分割成几十个芯片。相反，Cerebras 在整个晶圆上蚀刻了一个巨型芯片。

芯片越大，它可以拥有的内部内存就越多。其想法是拥有一个 SRAM 足够大的芯片来**容纳整个模型**，这样推理就可以完全在内存中发生。通常 GPU SRAM 以几十**兆字节**为单位测量。这意味着大量的推理时间花在将模型权重部分从 SRAM 外部流式传输到 GPU 计算单元上。如果你能从（快得多的）SRAM 流式传输所有这些，推理将获得巨大的速度提升：事实证明是 15 倍！

那么最新的 Cerebras 芯片有多少内部内存？44GB。这让 OpenAI 处于一个有点尴尬的位置。44GB 足以容纳一个小模型（fp16 下约 200 亿参数，int8 量化下约 400 亿参数），但显然不足以容纳 GPT-5.3-Codex。这就是为什么他们提供一个全新的模型，也是为什么 Spark 模型有点「小模型的味道」：它是更大的 GPT-5.3-Codex 模型的小型蒸馏版本。

## OpenAI 的版本在技术上更令人印象深刻

有趣的是，两个主要实验室有两种非常不同的方法来构建快速 AI 推理。如果我必须猜测一个阴谋论，它会是这样的：

- OpenAI 在一月中旬与 Cerebras 合作，显然是为了在快速 Cerebras 芯片上运行 OpenAI 模型
- Anthropic 没有类似的方案可用，但他们知道 OpenAI 将在二月份宣布某种超快推理，他们希望在新闻周期中有东西与之竞争
- Anthropic 因此匆忙组装了他们能够提供的快速推理：简单地在现有推理堆栈上降低批次大小
- Anthropic（可能）等到 OpenAI 完成他们复杂得多的 Cerebras 实现前几天才宣布，所以看起来 OpenAI 抄袭了他们

显然 OpenAI 在这里的成就在技术上更令人印象深刻。让模型在 Cerebras 芯片上运行并非易事，因为它们太奇怪了。训练一个 200 亿或 400 亿参数的 GPT-5.3-Codex 蒸馏版本，同时仍然「足够好」，也并非易事。但我赞扬 Anthropic 找到了一种偷偷摸摸的方法来领先于这个将对非技术人员来说很大程度上不透明的公告。这让我想起 OpenAI 在 2025 年中期偷偷引入 Responses API 来帮助他们隐藏推理 token 的做法。

## 快速 AI 推理是下一个大事件吗？

看到两个主要实验室推出这个功能可能会让你认为快速 AI 推理是他们追逐的新主要目标。我不认为是这样。如果我上面的理论是正确的，Anthropic 并没有那么关心快速推理，他们只是不想显得落后于 OpenAI。而 OpenAI 主要只是在探索他们新 Cerebras 合作伙伴关系的能力。什么样的模型可以装在这些巨型芯片上，这些模型有多大用处，以及经济学是否合理，这些都仍然是很大程度上未解的问题。

我个人不觉得「快速、能力较弱的推理」特别有用。我一直在 Codex 中试用它，我不喜欢它。AI agent 的有用性主要取决于**它们犯多少错误**，而不是它们的原始速度。以多犯 20% 错误为代价换取 6 倍速度是一笔糟糕的交易，因为用户的大部分时间都花在处理错误上，而不是等待模型。

然而，快速、能力较弱的推理当然有可能成为 AI 系统中的核心低级原语。Claude Code 已经使用 Haiku 进行某些操作。也许 OpenAI 最终会以类似的方式使用 Spark。

---

## 要点总结

1. **两种技术路线**：Anthropic 的快速模式通过降低批次大小实现（牺牲 GPU 吞吐量换取单个用户速度），OpenAI 则使用 Cerebras 的超大芯片（整个模型装入 SRAM）

2. **模型质量差异**：Anthropic 提供真正的 Opus 4.6，OpenAI 提供的是 Spark 蒸馏版（能力较弱）

3. **速度对比**：OpenAI 快 6 倍（1000 vs 170 token/s），但以模型能力为代价

4. **实际价值存疑**：对于 AI Agent，减少错误比提高速度更重要——处理错误的时间远超等待时间

5. **商业策略**：Anthropic 的快速模式可能更多是为了不在新闻上落后于 OpenAI 的 Cerebras 合作
