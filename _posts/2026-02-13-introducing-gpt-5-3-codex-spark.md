---
layout: post
title: "OpenAI 发布 GPT-5.3-Codex-Spark：首个实时编程模型"
date: 2026-02-13 21:59:57 +0800
categories: tech-translation
description: "OpenAI 与 Cerebras 合作推出 GPT-5.3-Codex-Spark，这是一款专为实时编程优化的小型模型，在超低延迟硬件上可实现每秒 1000+ tokens 的推理速度。"
original_url: https://openai.com/index/introducing-gpt-5-3-codex-spark/
source: Hacker News
---

本文翻译自 [Introducing GPT-5.3-Codex-Spark](https://openai.com/index/introducing-gpt-5-3-codex-spark/)，原载于 Hacker News。

---

OpenAI 今天发布了 GPT-5.3-Codex-Spark 的研究预览版。这是 GPT-5.3-Codex 的精简版本，也是首个专为**实时编程**设计的模型。Codex-Spark 标志着 OpenAI 与 Cerebras 合作的首个里程碑——这一合作在今年一月公布。

Codex-Spark 针对超低延迟硬件进行了深度优化，能够提供**每秒 1000+ tokens** 的推理速度，同时在处理真实编程任务时保持出色的能力。对于开发者来说，这意味着近乎即时的响应体验。

## 双模式编程体验

OpenAI 最新的前沿模型在**长时间运行任务**方面表现出色——能够自主工作数小时、数天甚至数周。而 Codex-Spark 则专注于另一个维度：与 Codex 进行**实时协作**。

现在，Codex 同时支持两种工作模式：
- **长时运行模式**：处理复杂、雄心勃勃的任务
- **实时交互模式**：进行针对性修改、重构逻辑、优化界面，并立即看到结果

## 技术规格

发布时的关键参数：
- **上下文窗口**：128k tokens
- **模态**：纯文本（暂时不支持图像）
- **速率限制**：独立配额，不计入标准限制

需要注意的是，在需求高峰期，可能会出现访问限制或临时排队。

## 速度与智能的平衡

Codex-Spark 针对延迟敏感的交互场景进行了优化。你可以：
- 与模型实时协作
- 在工作过程中随时打断或重定向
- 快速迭代，获得近乎即时的响应

由于专注于速度，Codex-Spark 采用了轻量级的默认工作风格：
- 只进行最小化、针对性的编辑
- 除非你明确要求，否则不会自动运行测试

## 性能表现

在评估 Agent 软件工程能力的两个基准测试中，Codex-Spark 展现了强劲的性能：

| 基准测试 | 表现 |
|---------|------|
| SWE-Bench Pro | 强劲性能 |
| Terminal-Bench 2.0 | 强劲性能 |

更重要的是，Codex-Spark 完成任务的时间仅为 GPT-5.3-Codex 的**一小部分**。

## 全链路延迟优化

在训练 Codex-Spark 的过程中，OpenAI 团队意识到模型速度只是实时协作的一部分——还需要降低整个请求-响应管道的延迟。他们对推理框架进行了全面优化，这些改进将惠及所有模型：

**核心改进：**
- 引入持久化 WebSocket 连接
- 针对 Responses API 进行定向优化

**性能提升：**
- 客户端/服务器往返开销降低 **80%**
- 每个 token 的开销降低 **30%**
- 首个 token 时间（TTFT）减少 **50%**

WebSocket 路径目前已为 Codex-Spark 默认启用，很快将成为所有模型的默认配置。

## Cerebras 驱动

Codex-Spark 运行在 Cerebras 的 **Wafer Scale Engine 3** 上——这是一款专为高速推理定制的 AI 加速器，为 Codex 提供了延迟优先的服务层级。

> "让我们最兴奋的是与 OpenAI 和开发者社区合作，探索快速推理带来的可能性——新的交互模式、新的使用场景，以及根本不同的模型体验。这次预览仅仅是个开始。"
> — Sean Lie，Cerebras CTO 兼联合创始人

**架构互补：**
- **GPU**：在训练和推理管道中保持基础地位，为广泛使用提供最具成本效益的 tokens
- **Cerebras**：在需要极低延迟的工作流中表现出色，让 Codex 在迭代过程中更加响应迅速

GPU 和 Cerebras 可以在单一工作负载中结合使用，以达到最佳性能。

## 如何获取

Codex-Spark 正在向 ChatGPT Pro 用户逐步推出，支持：
- 最新的 Codex 应用
- Codex CLI
- VS Code 扩展

API 访问目前仅面向少量设计合作伙伴开放，用于了解开发者如何将 Codex-Spark 集成到他们的产品中。未来几周将逐步扩大访问范围。

## 安全性

Codex-Spark 包含与主线模型相同的安全训练，包括网络安全相关训练。经过标准部署流程评估，该模型在网络安全或生物领域不具备达到高危能力阈值的可能性。

## 未来展望

Codex-Spark 是迈向双模式 Codex 的第一步：
1. **长周期推理与执行**
2. **快速迭代的实时协作**

随着时间推移，这两种模式将逐渐融合——Codex 可以让你保持紧密的交互循环，同时在后台将长时间运行的任务委托给子 Agent，或者在需要广度和速度时并行分配任务给多个模型。

**核心洞察：** 随着模型能力的提升，交互速度成为明显的瓶颈。超快速推理收紧了这个循环，让 Codex 使用起来更加自然，也为任何将想法转化为可工作软件的人扩展了可能性。

---

## 要点总结

1. **速度革命**：Codex-Spark 实现了每秒 1000+ tokens 的推理速度，为实时编程带来质变
2. **双模式协同**：长时自主任务 + 实时交互编辑，两种模式互补
3. **全链路优化**：不仅是模型快，WebSocket + API 优化让整体延迟降低 50-80%
4. **硬件创新**：Cerebras WSE-3 提供延迟优先的服务层级，与 GPU 形成互补
5. **渐进式发布**：ChatGPT Pro 用户优先体验，API 访问将逐步开放

对于国内开发者来说，这次发布最值得关注的是**交互式 AI 编程**的范式转变——从"提交-等待-接收"到"实时协作-即时反馈"。这可能预示着未来 IDE 集成的方向：AI 不再是后台的智能助手，而是与你并肩工作的实时配对编程伙伴。
