---
layout: post
title: "LLM 推理加速的两种不同路径"
date: 2026-02-16 00:50:15 +0800
categories: tech-translation
description: "Anthropic 和 OpenAI 最近都推出了「快速模式」，但实现方式截然不同：一个靠降低批量大小，一个靠巨型芯片。本文深入分析两种技术路线的优劣。"
original_url: https://www.seangoedecke.com/fast-llm-inference/
source: Hacker News
---

本文翻译自 [Two different tricks for fast LLM inference](https://www.seangoedecke.com/fast-llm-inference/)，原载于 Hacker News。

---

Anthropic 和 OpenAI 最近都宣布了「快速模式」（fast mode）：一种以显著更高的速度与其最强编程模型交互的方式。

这两种快速模式截然不同。Anthropic 提供高达 2.5 倍的 tokens/秒（约 170，相比 Opus 4.6 的 65）。OpenAI 则提供超过 1000 tokens/秒（相比 GPT-5.3-Codex 的 65 tokens/秒，提升 15 倍）。也就是说，OpenAI 的快速模式比 Anthropic 快六倍。

然而，Anthropic 的巨大优势在于他们提供的是真正的模型。使用他们的快速模式时，你得到的是真正的 Opus 4.6，而使用 OpenAI 的快速模式时，你得到的是 GPT-5.3-Codex-Spark，不是真正的 GPT-5.3-Codex。Spark 确实更快，但能力明显较弱：对于许多任务足够好用，但会在工具调用上犯错——这是原版 GPT-5.3-Codex 绝不会做的。

为什么会有这种差异？AI 实验室没有宣传他们快速模式的工作细节，但我相当确定是这样的：**Anthropic 的快速模式基于低批量大小（low-batch-size）推理，而 OpenAI 的快速模式基于特殊的巨型 Cerebras 芯片**。让我详细解释一下。

## Anthropic 的快速模式如何工作

AI 推理经济学核心的权衡是**批处理（batching）**，因为主要瓶颈是**内存**。GPU 非常快，但将数据移动到 GPU 上却不快。每次推理操作都需要在推理开始之前将用户 prompt 的所有 tokens 复制到 GPU 上。因此，将多个用户批处理起来会增加整体吞吐量，但代价是让用户等待批次填满。

一个很好的类比是公交系统。如果你对乘客零批处理——每当有人上车，公交车就立即出发——对于那些成功上车的人来说，通勤会快得多。但显然整体吞吐量会低得多，因为人们会在公交站等待数小时才能上车。

Anthropic 的快速模式本质上是一张公交车通行证，保证你一上车公交车就立即出发。价格是六倍，因为你实际上是在为所有其他本可以和你一起上车的人付费，但它快得多，因为你花费零时间等待公交车出发。

> 编辑注：有读者发邮件指出，「等待公交车」的成本实际上只影响第一个 token，因此不会影响流式延迟（只影响每轮或工具调用的延迟）。因此，最好将批量大小的性能影响理解为主要在于更小的批次需要更少的浮点运算，因此执行更快。在我的类比中，也许是「更轻的公交车跑得更快」之类的。

显然我无法完全确定这是正确的。也许他们有某种新的超快计算资源，或者他们在做一些别人没想到的算法技巧。但我相当确定就是这样。全新的计算资源或算法技巧可能需要改变模型（见下文 OpenAI 的系统），而「六倍价格换 2.5 倍速度提升」正是切换到低批量大小机制时你期望的那种改进范围。

## OpenAI 的快速模式如何工作

OpenAI 的快速模式完全不是这样工作的。你可以简单地从他们为此引入了一个新的、较弱的模型这一点看出来。如果他们只是调整批量大小，完全没有理由这样做。此外，他们在公告博客文章中明确告诉我们支持其快速模式的是什么：Cerebras。

OpenAI 在一月份一个月前宣布了与 Cerebras 的合作。Cerebras 是什么？他们构建「超低延迟计算」。实际上这意味着他们构建**巨型芯片**。一个 H100 芯片（相当接近推理芯片前沿）只有一平方英寸多一点。一个 Cerebras 芯片是 **70 平方英寸**。

![Cerebras 芯片](https://www.seangoedecke.com/static/a32e19a54795813e122dcbc1a5e013ef/1c72d/cerebras.jpg)

从图片中你可以看到 Cerebras 芯片上面有网格和孔洞图案。那是因为这么大的硅晶圆本应该被切割成几十个芯片。相反，Cerebras 在整个晶圆上蚀刻了一个巨型芯片。

芯片越大，它可以拥有的内部内存就越多。其想法是拥有一个 SRAM 足够大的芯片来容纳整个模型，这样推理就可以完全在内存中发生。通常 GPU SRAM 以几十**兆字节**为单位测量。这意味着很多推理时间花在将模型权重部分从 SRAM 外部流式传输到 GPU 计算。如果你能从（快得多的）SRAM 流式传输所有这些，推理将获得巨大的加速：事实证明是十五倍！

那么最新的 Cerebras 芯片有多少内部内存？44GB。这让 OpenAI 处于一个有点尴尬的位置。44GB 足以容纳一个小模型（fp16 下约 200 亿参数，int8 量化下约 400 亿参数），但显然不足以容纳 GPT-5.3-Codex。这就是为什么他们提供一个全新的模型，也是为什么 Spark 模型有一点「小模型的味道」：它是更大的 GPT-5.3-Codex 模型的小型蒸馏版本。

## OpenAI 的版本技术含量更高

有趣的是，两个主要实验室有两种截然不同的方法来构建快速 AI 推理。如果我必须猜测一个阴谋论，大概是这样的：

- OpenAI 在一月中旬与 Cerebras 合作，显然是为了在快速 Cerebras 芯片上运行 OpenAI 模型
- Anthropic 没有类似的方案，但他们知道 OpenAI 将在二月份宣布某种超快推理，他们想在新闻周期中有东西与之竞争
- Anthropic 因此匆忙组装了他们**可以**提供的那种快速推理：简单地在他们现有的推理堆栈上降低批量大小
- Anthropic（可能）等到 OpenAI 完成他们复杂得多的 Cerebras 实现前几天才宣布，所以看起来像是 OpenAI 抄袭了他们

显然 OpenAI 的成就在技术上更令人印象深刻。让模型在 Cerebras 芯片上运行并不简单，因为它们太奇怪了。训练一个 200 亿或 400 亿参数的 GPT-5.3-Codex 蒸馏版本，使其仍然够好，也不简单。但我赞扬 Anthropic 找到了一种狡猾的方式来领先于公告，这对非技术人员来说很大程度上是不透明的。这让我想起了 OpenAI 2025 年年中狡猾地引入 Responses API 来帮助他们隐藏推理 tokens。

## 快速 AI 推理是下一个大事物吗？

看到两个主要实验室推出这个功能可能会让你认为快速 AI 推理是他们追逐的新的主要目标。我不这么认为。如果我上面的理论是正确的，Anthropic 并没有**那么**关心快速推理，他们只是不想显得落后于 OpenAI。而 OpenAI 主要只是在探索他们新的 Cerebras 合作伙伴关系的能力。什么样的模型可以放在这些巨型芯片上，这些模型有多大用处，以及经济学是否有意义，这仍然是一个很大程度上悬而未决的问题。

我个人不觉得「快速、能力较弱的推理」特别有用。我一直在 Codex 中尝试它，我不喜欢它。AI agents 的有用性主要取决于**它们犯多少错误**，而不是它们的原始速度。用 6 倍速度换取多 20% 的错误是一个糟糕的交易，因为用户的大部分时间都花在处理错误上，而不是等待模型。

然而，快速、能力较弱的推理当然有可能成为 AI 系统中的核心底层原语。Claude Code 已经在某些操作中使用 Haiku。也许 OpenAI 最终会以类似的方式使用 Spark。

---

## 要点总结

1. **两种技术路线**：Anthropic 通过降低批处理大小来加速（贵但用完整模型），OpenAI 通过 Cerebras 巨型芯片实现全内存推理（快但用小模型）
2. **速度 vs 质量**：OpenAI 快 6 倍但模型能力下降；Anthropic 保持完整模型但速度提升有限
3. **核心瓶颈**：内存带宽是推理速度的主要限制，而非计算能力
4. **实用考量**：对 AI agents 来说，减少错误比提升速度更重要——大部分时间花在修 bug 上
5. **未来趋势**：快速小模型可能成为 AI 系统的底层原语，用于处理简单任务
